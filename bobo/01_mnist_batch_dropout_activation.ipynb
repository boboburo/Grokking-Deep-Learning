{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1966238-8ac6-4c46-a365-7b83129a0e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f2ea0b-c6dc-4dc6-802a-76211251c4f3",
   "metadata": {},
   "source": [
    "# Read in Data\n",
    "\n",
    "The MNIST dataset is contained in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc07da5c-cba8-4ddc-ba93-534b3948f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import numpy as np \n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0590c71f-dd35-48a3-aa82-80ae645f72c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7d6ab11-846b-4b5d-a34d-f818acff46d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phsp(x, x_name = None):\n",
    "    if x_name != None:\n",
    "        print(f\"{x_name} is of shape {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a11802f-4f7a-484e-85a0-c8afc8835fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train is of shape (60000, 28, 28)\n",
      "y_train is of shape (60000,)\n",
      "x_test is of shape (10000, 28, 28)\n",
      "y_test is of shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "phsp(X_train, \"x_train\")\n",
    "phsp(y_train, \"y_train\")\n",
    "phsp(X_test, \"x_test\")\n",
    "phsp(y_test, \"y_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966d545-49b0-4d76-9492-8e2b140ba97b",
   "metadata": {},
   "source": [
    "## Filter to NUM_EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aad23e18-cdb4-423e-94e8-cac8ba8a8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d056bed7-5875-4258-ab83-ab46b012cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = X_train[0:NUM_EXAMPLES].reshape(NUM_EXAMPLES, 28*28)/255\n",
    "train_lbls = y_train[0:NUM_EXAMPLES]\n",
    "\n",
    "test_imgs = X_test.reshape(len(X_test), 28*28)/255\n",
    "test_lbls = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8bdc27a-8815-4825-b357-7524abe2feba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images is of shape (1000, 784)\n",
      "labels is of shape (1000,)\n"
     ]
    }
   ],
   "source": [
    "phsp(train_imgs,\"train_images\")\n",
    "phsp(train_lbls, \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a239be7-d71a-4363-ac17-012e041ae511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rows, columns\n",
    "train_imgs[0:5, 0:2] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e7c960-156b-42c0-8140-d7f12c9a25b4",
   "metadata": {},
   "source": [
    "## make the labels one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3d69d82-0d79-4549-8b49-dd4a532af3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_lbls_ohe is of shape (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "ohe = np.zeros((len(train_lbls), 10))\n",
    "phsp(ohe,\"train_lbls_ohe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca23d56e-8cdf-4ef0-8057-2450dde38554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe[0:5,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07cab48f-1f9e-4474-b40c-2bb8adb76f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,l in enumerate(train_lbls):\n",
    "    ohe[i][l] = 1\n",
    "\n",
    "ohe[0:5,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fdbafea-a193-4500-8192-0e8c42d90b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_lbls is of shape (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_lbls = ohe\n",
    "phsp(train_lbls, \"train_lbls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa5ee96-e4a4-488c-a99c-9036b1caca04",
   "metadata": {},
   "source": [
    "## same for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3183b07a-5a5a-438e-a932-21ea330fc32d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_lbls is of shape (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "ohe = np.zeros((len(test_lbls), 10))\n",
    "for i, l in enumerate(test_lbls):\n",
    "    ohe[i][l] = 1\n",
    "\n",
    "test_lbls = ohe\n",
    "phsp(test_lbls, \"test_lbls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a58ee7-d2a4-4055-ac09-f78453dc01ef",
   "metadata": {},
   "source": [
    "# Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "71327efe-9a83-4fb5-88e2-bfe022f311b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "relu = lambda x:(x>=0)*x\n",
    "relu2deriv = lambda x: x>=0\n",
    "\n",
    "batch_size = 100\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 300, 40, 784, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "97e3c42c-a6a3-4c0d-aa9e-ef11687b4555",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_0_1 = 0.2*np.random.random((pixels_per_image, hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dfc461e8-d172-4e09-b8ea-e7e39a53b5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07666122,  0.02473444,  0.05018849],\n",
       "       [-0.09602397, -0.0947578 , -0.0943387 ],\n",
       "       [-0.05203045, -0.00124606,  0.02399114]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_0_1[2:5, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf6a13b-572d-465b-afb1-ace14610f20c",
   "metadata": {},
   "source": [
    "## Naive Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6e8768de-a2cf-422f-9db7-ce0a4f67df0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:0, Train Error:0.722, Train Correct:0.537\n",
      "Iteration:50, Train Error:0.204, Train Correct:0.966\n",
      "Iteration:100, Train Error:0.167, Train Correct:0.984\n",
      "Iteration:150, Train Error:0.145, Train Correct:0.991\n",
      "Iteration:200, Train Error:0.130, Train Correct:0.998\n",
      "Iteration:250, Train Error:0.120, Train Correct:0.999\n",
      "Iteration:299, Train Error:0.113, Train Correct:0.999\n"
     ]
    }
   ],
   "source": [
    "#without drop out\n",
    "\n",
    "for j in range(iterations):\n",
    "    error, correct_cnt = (0.0, 0)\n",
    "    \n",
    "    for i in range(len(train_imgs)):\n",
    "        layer_0 = train_imgs[i:i+1] #single row (1,784)\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1)) # (1,40)\n",
    "        layer_2 = np.dot(layer_1, weights_1_2) # (1,10)\n",
    "        \n",
    "        error = error + np.sum((train_lbls[i:i+1] - layer_2) **2)\n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(train_lbls[i:i+1]))\n",
    "        \n",
    "        layer_2_delta = (train_lbls[i:i+1] - layer_2)\n",
    "        layer_1_delta = np.dot(layer_2_delta, weights_1_2.T) * relu2deriv(layer_1) #take the dir of the original output\n",
    "        \n",
    "        weights_1_2 += alpha*(np.dot(layer_1.T, layer_2_delta)) #same as np.dot\n",
    "        weights_0_1 += alpha*(np.dot(layer_0.T, layer_1_delta))\n",
    "    \n",
    "    if(j % 50 == 0 or j == iterations -1):\n",
    "        print(f\"Iteration:{j}, Train Error:{error/float(len(train_imgs)):.3f}, Train Correct:{correct_cnt/float(len(train_imgs))}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e34009-d9d6-45dc-b4ad-69ecf1ec2957",
   "metadata": {},
   "source": [
    "## Test Accuracy\n",
    "\n",
    "Shows overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2fbc94ec-54c7-4707-9a14-0e2e9ad5c643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error:0.614, Test Correct:0.718\n"
     ]
    }
   ],
   "source": [
    "## try on test\n",
    "error, correct_cnt = (0.0, 0)\n",
    "\n",
    "for i in range(len(test_imgs)):\n",
    "    layer_0 = test_imgs[i:i+1]\n",
    "    layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "    layer_2 = np.dot(layer_1, weights_1_2)\n",
    "    \n",
    "    error = error + np.sum((test_lbls[i:i+1] - layer_2)**2)\n",
    "    correct_cnt += int(np.argmax(layer_2) == np.argmax(test_lbls[i:i+1]))\n",
    "    \n",
    "print(f\"Test Error:{error/float(len(test_imgs)):.3f}, Test Correct:{correct_cnt/float(len(test_imgs)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a82952-375d-42a5-b3f1-98181ad147b9",
   "metadata": {},
   "source": [
    "# Add dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "297fa2fc-e286-4b66-a5cb-b884d6e9be46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:0, Train Error:0.606, Train Correct:0.655\n",
      "Iteration:50, Train Error:0.435, Train Correct:0.767\n",
      "Iteration:100, Train Error:0.437, Train Correct:0.782\n",
      "Iteration:150, Train Error:0.415, Train Correct:0.811\n",
      "Iteration:200, Train Error:0.394, Train Correct:0.828\n",
      "Iteration:250, Train Error:0.380, Train Correct:0.838\n",
      "Iteration:299, Train Error:0.373, Train Correct:0.84\n"
     ]
    }
   ],
   "source": [
    "#without drop out\n",
    "\n",
    "for j in range(iterations):\n",
    "    error, correct_cnt = (0.0, 0)\n",
    "    \n",
    "    for i in range(len(train_imgs)):\n",
    "        layer_0 = train_imgs[i:i+1] #single row (1,784)\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1)) # (1,40)\n",
    "        \n",
    "        #####################################################\n",
    "        dropout_mask = np.random.randint(2, size  = layer_1.shape) #0 or 1 for (1,40)\n",
    "        layer_1 = layer_1*dropout_mask * 2 #randomly turn on off nodes, make sure to amplify volume of remaining\n",
    "        #######################################################\n",
    "        \n",
    "        \n",
    "        layer_2 = np.dot(layer_1, weights_1_2) # (1,10)\n",
    "        \n",
    "        error = error + np.sum((train_lbls[i:i+1] - layer_2) **2)\n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(train_lbls[i:i+1]))\n",
    "        \n",
    "        layer_2_delta = (train_lbls[i:i+1] - layer_2)\n",
    "        layer_1_delta = (np.dot(layer_2_delta, weights_1_2.T)) * relu2deriv(layer_1) #take the dir of the original output\n",
    "        \n",
    "        ####################################################\n",
    "        layer_1_delta = layer_1_delta * dropout_mask #turn off those that were original turned off, can't update these weights\n",
    "        \n",
    "        weights_1_2 += alpha*(np.dot(layer_1.T, layer_2_delta)) #same as np.dot\n",
    "        weights_0_1 += alpha*(np.dot(layer_0.T, layer_1_delta))\n",
    "    \n",
    "    if(j % 50 == 0 or j == iterations -1):\n",
    "        print(f\"Iteration:{j}, Train Error:{error/float(len(train_imgs)):.3f}, Train Correct:{correct_cnt/float(len(train_imgs))}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b1a2d3cf-9c49-466c-82e7-a5131f1d42e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error:0.418, Test Correct:0.813\n"
     ]
    }
   ],
   "source": [
    "## try on test\n",
    "error, correct_cnt = (0.0, 0)\n",
    "\n",
    "for i in range(len(test_imgs)):\n",
    "    layer_0 = test_imgs[i:i+1]\n",
    "    layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "    layer_2 = np.dot(layer_1, weights_1_2)\n",
    "    \n",
    "    error = error + np.sum((test_lbls[i:i+1] - layer_2)**2)\n",
    "    correct_cnt += int(np.argmax(layer_2) == np.argmax(test_lbls[i:i+1]))\n",
    "    \n",
    "print(f\"Test Error:{error/float(len(test_imgs)):.3f}, Test Correct:{correct_cnt/float(len(test_imgs)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9984c-033b-40a1-a8b8-ba95defdcd4b",
   "metadata": {},
   "source": [
    "## Add batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e7e95276-6784-41de-9f44-bb656de03e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "alpha, iterations = (0.001, 300)\n",
    "pixels_per_image, num_labels, hidden_size = (784, 10, 100)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image, hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8e82391d-c016-4b26-b918-da45fb6c1657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:0, \n",
      "                  Train Error:1.196,\n",
      "                  Train Correct:0.168,\n",
      "                  Test Error: 0.824656,\n",
      "                  Test Correct: 0.3782\n",
      "Iteration:50, \n",
      "                  Train Error:0.453,\n",
      "                  Train Correct:0.797,\n",
      "                  Test Error: 0.439282,\n",
      "                  Test Correct: 0.8059\n",
      "Iteration:100, \n",
      "                  Train Error:0.426,\n",
      "                  Train Correct:0.816,\n",
      "                  Test Error: 0.431470,\n",
      "                  Test Correct: 0.8128\n",
      "Iteration:150, \n",
      "                  Train Error:0.391,\n",
      "                  Train Correct:0.837,\n",
      "                  Test Error: 0.427535,\n",
      "                  Test Correct: 0.8174\n",
      "Iteration:200, \n",
      "                  Train Error:0.382,\n",
      "                  Train Correct:0.849,\n",
      "                  Test Error: 0.425232,\n",
      "                  Test Correct: 0.802\n",
      "Iteration:250, \n",
      "                  Train Error:0.382,\n",
      "                  Train Correct:0.842,\n",
      "                  Test Error: 0.422566,\n",
      "                  Test Correct: 0.8029\n",
      "Iteration:299, \n",
      "                  Train Error:0.358,\n",
      "                  Train Correct:0.867,\n",
      "                  Test Error: 0.416477,\n",
      "                  Test Correct: 0.8003\n"
     ]
    }
   ],
   "source": [
    "for j in range(iterations):\n",
    "    error, correct_cnt = (0.0, 0)\n",
    "    \n",
    "    for i in range(int(len(train_imgs) / batch_size)):\n",
    "        \n",
    "        batch_start, batch_end = ((i * batch_size),((i+1)*batch_size))\n",
    "        \n",
    "        layer_0 = train_imgs[batch_start:batch_end] #single row (1,784) or with batch (100, 784)\n",
    "        \n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1)) # (1,40) \n",
    "        dropout_mask = np.random.randint(2, size  = layer_1.shape) \n",
    "        layer_1 = layer_1*dropout_mask * 2 \n",
    "        \n",
    "        layer_2 = np.dot(layer_1, weights_1_2) # (1,10)\n",
    "        \n",
    "        error = error + np.sum((train_lbls[batch_start:batch_end] - layer_2)**2)\n",
    "\n",
    "        layer_2_delta = (train_lbls[batch_start:batch_end] - layer_2)\n",
    "        \n",
    "        layer_1_delta = (np.dot(layer_2_delta, weights_1_2.T)) * relu2deriv(layer_1) #take the dir of the original output\n",
    "        layer_1_delta = layer_1_delta * dropout_mask\n",
    "        \n",
    "        weights_1_2 += alpha*(np.dot(layer_1.T, layer_2_delta)) #same as np.dot\n",
    "        weights_0_1 += alpha*(np.dot(layer_0.T, layer_1_delta))\n",
    "        \n",
    "        for k in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(train_lbls[batch_start+k:batch_start+k+1]))\n",
    "        \n",
    "    \n",
    "    if(j % 50 == 0 or j == iterations -1):\n",
    "        \n",
    "        test_error = 0.0\n",
    "        test_correct_cnt = 0\n",
    "\n",
    "        for i in range(len(test_imgs)):\n",
    "            layer_0 = test_imgs[i:i+1]\n",
    "            layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "            test_error += np.sum((test_lbls[i:i+1] - layer_2) ** 2)\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_lbls[i:i+1]))\n",
    "\n",
    "        print(f'''Iteration:{j}, \n",
    "                  Train Error:{error/float(len(train_imgs)):.3f},\n",
    "                  Train Correct:{correct_cnt/float(len(train_imgs))},\n",
    "                  Test Error: {test_error/float(len(test_imgs)):3f},\n",
    "                  Test Correct: {test_correct_cnt/float(len(test_imgs))}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529c9972-856e-4d2f-a35f-fea210ea7455",
   "metadata": {},
   "source": [
    "#### New Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fa61f5b-d0be-488a-87a1-91483dd40d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh = lambda x: np.tanh(x)\n",
    "tanh2deriv = lambda x: (1-(x**2))\n",
    "\n",
    "def softmax(x):\n",
    "    tmp = np.exp(x)\n",
    "    return tmp / np.sum(tmp, axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "316f55ed-7e51-4e34-a8e8-de14be8ee8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "#### alpha is much higher ? \n",
    "alpha, iterations, hidden_size = (2, 300, 100)\n",
    "pixels_per_image, num_labelse = (784, 10)\n",
    "\n",
    "###### adjusted to be between -0.01 and 0.01 \n",
    "weights_0_1 = 0.02*np.random.random((pixels_per_image, hidden_size)) - 0.01\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ac1089d-7bd0-4c50-b813-9eac96e5fb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:0, \n",
      "                  Train Accuracy:0.141,\n",
      "                  Test Accuracy: 0.2986\n",
      "Iteration:50, \n",
      "                  Train Accuracy:0.829,\n",
      "                  Test Accuracy: 0.7974\n",
      "Iteration:100, \n",
      "                  Train Accuracy:0.878,\n",
      "                  Test Accuracy: 0.8398\n",
      "Iteration:150, \n",
      "                  Train Accuracy:0.912,\n",
      "                  Test Accuracy: 0.8568\n",
      "Iteration:200, \n",
      "                  Train Accuracy:0.931,\n",
      "                  Test Accuracy: 0.8633\n",
      "Iteration:250, \n",
      "                  Train Accuracy:0.938,\n",
      "                  Test Accuracy: 0.8682\n",
      "Iteration:299, \n",
      "                  Train Accuracy:0.954,\n",
      "                  Test Accuracy: 0.8727\n"
     ]
    }
   ],
   "source": [
    "for j in range(iterations):\n",
    "    error, correct_cnt = (0.0, 0)\n",
    "    \n",
    "    for i in range(int(len(train_imgs) / batch_size)):\n",
    "        \n",
    "        batch_start, batch_end = ((i * batch_size),((i+1)*batch_size))\n",
    "        \n",
    "        layer_0 = train_imgs[batch_start:batch_end] #single row (1,784) or with batch (100, 784)\n",
    "        \n",
    "        layer_1 = tanh(np.dot(layer_0, weights_0_1)) # (1,40) \n",
    "        dropout_mask = np.random.randint(2, size  = layer_1.shape) \n",
    "        layer_1 = layer_1*dropout_mask * 2 \n",
    "        \n",
    "        layer_2 = softmax(np.dot(layer_1, weights_1_2)) # (1,10)\n",
    "        \n",
    "        \n",
    "        ##############################\n",
    "        layer_2_delta = (train_lbls[batch_start:batch_end] - layer_2)/ (batch_size * layer_2.shape[0])\n",
    "        ##############################\n",
    "        \n",
    "        layer_1_delta = (np.dot(layer_2_delta, weights_1_2.T)) * tanh2deriv(layer_1) #take the dir of the original output\n",
    "        layer_1_delta = layer_1_delta * dropout_mask\n",
    "        \n",
    "        weights_1_2 += alpha*(np.dot(layer_1.T, layer_2_delta)) #same as np.dot\n",
    "        weights_0_1 += alpha*(np.dot(layer_0.T, layer_1_delta))\n",
    "        \n",
    "        for k in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(train_lbls[batch_start+k:batch_start+k+1]))\n",
    "        \n",
    "    \n",
    "    if(j % 50 == 0 or j == iterations -1):\n",
    "        \n",
    "        test_correct_cnt = 0\n",
    "\n",
    "        for i in range(len(test_imgs)):\n",
    "            layer_0 = test_imgs[i:i+1]\n",
    "            ###########################################\n",
    "            layer_1 = tanh(np.dot(layer_0,weights_0_1))\n",
    "            ###########################################\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_lbls[i:i+1]))\n",
    "\n",
    "        print(f'''Iteration:{j}, \n",
    "                  Train Accuracy:{correct_cnt/float(len(train_imgs))},\n",
    "                  Test Accuracy: {test_correct_cnt/float(len(test_imgs))}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe91442-a833-4edb-a35c-29494c2dfa81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
