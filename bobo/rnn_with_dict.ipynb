{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "f = open('../original/tasksv11/en/qa1_single-supporting-fact_train.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', 'moved', 'to', 'the', 'bathroom']\n",
      "['john', 'went', 'to', 'the', 'hallway']\n",
      "['where', 'is', 'mary', 'bathroom']\n",
      "['daniel', 'went', 'back', 'to', 'the', 'hallway']\n",
      "['sandra', 'moved', 'to', 'the', 'garden']\n"
     ]
    }
   ],
   "source": [
    "for line in raw[0:5]:\n",
    "    print(line.lower().\n",
    "          replace(\"\\n\",\"\").\n",
    "          replace(\"\\t\",\"\").\n",
    "          replace(\".\",\"\").\n",
    "          replace(\"?\",\"\").\n",
    "          replace(\"1\",\"\").split(\" \")[1:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary', 'moved', 'to', 'the', 'bathroom'], ['john', 'went', 'to', 'the', 'hallway'], ['where', 'is', 'mary', 'bathroom'], ['daniel', 'went', 'back', 'to', 'the', 'hallway'], ['sandra', 'moved', 'to', 'the', 'garden']]\n"
     ]
    }
   ],
   "source": [
    "tokens = list()\n",
    "for line in raw[0:5]:\n",
    "    tokens.append(line.lower().\n",
    "          replace(\"\\n\",\"\").\n",
    "          replace(\"\\t\",\"\").\n",
    "          replace(\".\",\"\").\n",
    "          replace(\"?\",\"\").\n",
    "          replace(\"1\",\"\").split(\" \")[1:]\n",
    "                 )\n",
    "    \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "    \n",
    "def words2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print(sum([len(x) for x in tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "embed_size = 10\n",
    "\n",
    "# word embeddings\n",
    "embed = (np.random.rand(len(vocab),embed_size) - 0.5) * 0.1 #[14,10]\n",
    "\n",
    "# embedding -> embedding (initially the identity matrix)\n",
    "recurrent = np.eye(embed_size) #[10,10]\n",
    "\n",
    "# sentence embedding for empty sentence\n",
    "start = np.zeros(embed_size) #[10,]\n",
    "\n",
    "# embedding -> output weights\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1 #[10,14]\n",
    "\n",
    "# one hot lookups (for loss function)\n",
    "one_hot = np.eye(len(vocab)) #[14,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sent):\n",
    "    \n",
    "    layers = list()\n",
    "    layer = {}\n",
    "    layer['hidden'] = start\n",
    "    layers.append(layer)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # forward propagate\n",
    "    preds = list()\n",
    "    for target_i in range(len(sent)):\n",
    "\n",
    "        layer = {}\n",
    "\n",
    "        # try to predict the next term\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder)) #[10,][10,14] = [1,19]\n",
    "\n",
    "        loss += -np.log(layer['pred'][sent[target_i]]) #will be zero if 1\n",
    "\n",
    "        # generate the next hidden state\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[sent[target_i]] #[10,1][10,10] + [1,10]\n",
    "        layers.append(layer)\n",
    "        \n",
    "    return layers, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john', 'went', 'to', 'the', 'hallway']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['went', 'to', 'the', 'hallway']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 6, 5, 12]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_sent = words2indices(tokens[1][1:])\n",
    "tmp_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer,_ = predict([13,6,5,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'hidden': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " {'pred': array([0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,\n",
       "         0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,\n",
       "         0.07142857, 0.07142857, 0.07142857, 0.07142857]),\n",
       "  'hidden': array([ 0.00857593,  0.04695957,  0.00610302, -0.04813527,  0.03006327,\n",
       "         -0.02670257,  0.03071052, -0.01121394,  0.03635419,  0.02471216])},\n",
       " {'pred': array([0.0710932 , 0.07162197, 0.07144105, 0.07157902, 0.07184681,\n",
       "         0.07153482, 0.07137346, 0.07120162, 0.07166611, 0.07176285,\n",
       "         0.07120926, 0.0712069 , 0.07125751, 0.07120542]),\n",
       "  'hidden': array([-0.03119063,  0.03836517,  0.02554304, -0.05671734, -0.01494139,\n",
       "         -0.02311293,  0.04708998, -0.00972502,  0.08081366,  0.03336767])},\n",
       " {'pred': array([0.07112876, 0.07197887, 0.07149297, 0.07165562, 0.07181289,\n",
       "         0.07158648, 0.07119637, 0.07107706, 0.07179413, 0.07189793,\n",
       "         0.07122969, 0.07105498, 0.07108696, 0.0710073 ]),\n",
       "  'hidden': array([-0.07925393,  0.05624873, -0.00329415, -0.08016268, -0.01578407,\n",
       "         -0.06777668,  0.05450174, -0.04505217,  0.08974421,  0.0533435 ])},\n",
       " {'pred': array([0.07098777, 0.07192759, 0.07173967, 0.07160183, 0.07208114,\n",
       "         0.0717606 , 0.07123363, 0.07062316, 0.07181404, 0.07235665,\n",
       "         0.07132476, 0.07068422, 0.07104232, 0.07082264]),\n",
       "  'hidden': array([-0.12726592,  0.00886983, -0.0504635 , -0.10554157,  0.02021872,\n",
       "         -0.06389357,  0.05978394, -0.01084908,  0.05216155,  0.03126187])}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred': array([0.0710932 , 0.07162197, 0.07144105, 0.07157902, 0.07184681,\n",
       "        0.07153482, 0.07137346, 0.07120162, 0.07166611, 0.07176285,\n",
       "        0.07120926, 0.0712069 , 0.07125751, 0.07120542]),\n",
       " 'hidden': array([-0.03119063,  0.03836517,  0.02554304, -0.05671734, -0.01494139,\n",
       "        -0.02311293,  0.04708998, -0.00972502,  0.08081366,  0.03336767])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step0 \n",
    "h0 = start\n",
    "#step1\n",
    "p1 = softmax(h0.dot(decoder)) #equal odds for all words\n",
    "h1 = h0.dot(recurrent) + embed[tmp_sent[0]] #random vector weights one row for each token\n",
    "#step2\n",
    "p2 = softmax(h1.dot(decoder))\n",
    "h2 = h1.dot(recurrent) + embed[tmp_sent[1]]\n",
    "#and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward\n",
    "for iter in range(1):\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[iter%len(tokens)][1:]) #Sentence less the first word, with an interator \n",
    "    \n",
    "    layers,loss = predict(sent) #returns multiple pred layers for each word [19,]\n",
    "\n",
    "    # back propagate\n",
    "    for layer_idx in reversed(range(len(layers))):\n",
    "        layer = layers[layer_idx]\n",
    "        target = sent[layer_idx-1] #because the sent is reduced by 1 in length and there is a start layer added when \n",
    "        #passed to predict(sent)\n",
    "\n",
    "        if(layer_idx > 0):  # if not the first layer\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target] #takes a particular row away [19,]\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose()) #[19,][10,19] = [10,]\n",
    "            \n",
    "            # if the last layer - don't pull from a later one becasue it doesn't exist\n",
    "            if(layer_idx == len(layers)-1):\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
    "        else: # if the first layer\n",
    "            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 14)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.outer(layers[2]['hidden'],layers[2]['output_delta']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'hidden': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  'hidden_delta': array([ 0.01240052,  0.01205607,  0.02418878, -0.05836742, -0.02181459,\n",
       "         -0.00636029, -0.0034963 , -0.00752625,  0.03472336,  0.04612733])},\n",
       " {'pred': array([0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,\n",
       "         0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,\n",
       "         0.07142857, 0.07142857, 0.07142857, 0.07142857]),\n",
       "  'hidden': array([ 0.04034019, -0.03625253, -0.03607237,  0.03073913, -0.01023232,\n",
       "         -0.03346458,  0.04275086, -0.01522341,  0.02508121,  0.0225998 ]),\n",
       "  'output_delta': array([ 0.07142857,  0.07142857,  0.07142857,  0.07142857,  0.07142857,\n",
       "          0.07142857,  0.07142857, -0.92857143,  0.07142857,  0.07142857,\n",
       "          0.07142857,  0.07142857,  0.07142857,  0.07142857]),\n",
       "  'hidden_delta': array([ 0.01240052,  0.01205607,  0.02418878, -0.05836742, -0.02181459,\n",
       "         -0.00636029, -0.0034963 , -0.00752625,  0.03472336,  0.04612733])},\n",
       " {'pred': array([0.07152756, 0.0711993 , 0.0714539 , 0.07137798, 0.07120131,\n",
       "         0.07139787, 0.07139547, 0.07137505, 0.07147454, 0.071286  ,\n",
       "         0.07156653, 0.07179152, 0.07158384, 0.07136913]),\n",
       "  'hidden': array([ 0.00057363, -0.04484693, -0.01663235,  0.02215706, -0.05523697,\n",
       "         -0.02987494,  0.05913032, -0.0137345 ,  0.06954069,  0.0312553 ]),\n",
       "  'output_delta': array([ 0.07152756,  0.0711993 ,  0.0714539 ,  0.07137798,  0.07120131,\n",
       "          0.07139787, -0.92860453,  0.07137505,  0.07147454,  0.071286  ,\n",
       "          0.07156653,  0.07179152,  0.07158384,  0.07136913]),\n",
       "  'hidden_delta': array([ 0.05264501,  0.01497535,  0.04909514, -0.02190823, -0.02017855,\n",
       "          0.04872572, -0.00803836, -0.00103694,  0.02835127,  0.03841301])},\n",
       " {'pred': array([0.07156375, 0.07155451, 0.07150625, 0.07145477, 0.07116812,\n",
       "         0.07144984, 0.07121874, 0.0712506 , 0.07160263, 0.0714206 ,\n",
       "         0.07158748, 0.07163877, 0.07141293, 0.07117098]),\n",
       "  'hidden': array([-0.04748967, -0.02696338, -0.04546954, -0.00128828, -0.05607965,\n",
       "         -0.07453869,  0.06654208, -0.04906165,  0.07847124,  0.05123114]),\n",
       "  'output_delta': array([ 0.07156375,  0.07155451,  0.07150625,  0.07145477,  0.07116812,\n",
       "         -0.92855016,  0.07121874,  0.0712506 ,  0.07160263,  0.0714206 ,\n",
       "          0.07158748,  0.07163877,  0.07141293,  0.07117098]),\n",
       "  'hidden_delta': array([ 0.04411941, -0.00744456,  0.0545721 , -0.02549927,  0.00886767,\n",
       "          0.01118004, -0.02120437,  0.02304853, -0.00814465,  0.04987155])},\n",
       " {'pred': array([0.07142265, 0.07150429, 0.07175375, 0.07140188, 0.0714347 ,\n",
       "         0.07162438, 0.07125676, 0.07079633, 0.07162325, 0.07187703,\n",
       "         0.07168379, 0.07126571, 0.07136884, 0.07098663]),\n",
       "  'hidden': array([-0.06482518, -0.02425757, -0.00687533, -0.0155613 , -0.01522614,\n",
       "         -0.06220267,  0.01812421, -0.00611792,  0.09756093,  0.10096342]),\n",
       "  'output_delta': array([ 0.07142265,  0.07150429,  0.07175375,  0.07140188,  0.0714347 ,\n",
       "          0.07162438,  0.07125676,  0.07079633,  0.07162325,  0.07187703,\n",
       "         -0.92831621,  0.07126571,  0.07136884,  0.07098663]),\n",
       "  'hidden_delta': array([ 0.02379766,  0.03080172,  0.04296396, -0.0175658 ,  0.00205374,\n",
       "          0.02203275,  0.01015549, -0.01324853, -0.01041598,  0.03087514])}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
