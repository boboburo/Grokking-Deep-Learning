{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "f = open(\"../original/reviews.txt\")\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "print(type(raw_reviews))\n",
    "print(len(raw_reviews))\n",
    "\n",
    "f = open(\"../original/labels.txt\")\n",
    "raw_labels = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2220 characters in this review.\n",
      "Composed of 472 words defined by space.\n",
      "Composed of 224 unique words\n"
     ]
    }
   ],
   "source": [
    "#practice\n",
    "tmp_review = raw_reviews[234]\n",
    "print(f\"There are {len(tmp)} characters in this review.\")\n",
    "tmp_words = tmp_review.split(\" \")\n",
    "print(f\"Composed of {len(tmp_words)} words defined by space.\")\n",
    "tmp_unique_words = set(tmp_words)\n",
    "print(f\"Composed of {len(tmp_unique_words)} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "tokens = list(map(lambda x:set(x.split(\" \")), raw_reviews))\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 74074 unique words across all 25000 reviews in the dataset.\n"
     ]
    }
   ],
   "source": [
    "#Get the unique vocab\n",
    "vocab = set()\n",
    "for review in tokens:\n",
    "    for word in review:\n",
    "        if(len(word)>0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "print(f\"There are {len(vocab)} unique words across all {len(tokens)} reviews in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16240"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Give it numbers\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "#each word gets a number\n",
    "word2index[\"brave\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#covert review to a set of unique numbers representing the words in it. \n",
    "input_dataset = list()\n",
    "for review in tokens:\n",
    "    review_index = list()\n",
    "    for word in review:\n",
    "        try:\n",
    "            review_index.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(list(set(review_index)))\n",
    "len(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27140, 41479, 1042, 40467]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explainer, input dataset - just numbers related to words\n",
    "input_dataset[342][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = list()\n",
    "for label in raw_labels:\n",
    "    if label == \"positive\\n\":\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "alpha, iterations = (0.01, 2)\n",
    "hidden_size = 100\n",
    "\n",
    "w01 = 0.2*np.random.random((len(vocab), hidden_size)) - 0.1 #74074, 100\n",
    "w12 = 0.2*np.random.random((hidden_size, 1)) - 0.1\n",
    "\n",
    "correct, total = (0,0)\n",
    "\n",
    "sigmoid = lambda x: 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.26509341, 0.69362389, 0.63203359, 0.75871518])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explainer, the input dataset contains list of number related to a word \n",
    "#rather than multiple can just add the weights\n",
    "\n",
    "tmp_input = input_dataset[342][0:4]\n",
    "print(w01[tmp_input].shape) #grab 4 of the weight vectors \n",
    "tmp_sum = np.sum(w01[tmp_input], axis = 1)\n",
    "sigmoid(tmp_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 9, Trn-Error: [0.34185575], Correct: 4, Train-Acc: 0.400\n",
      "Iter: 1009, Trn-Error: [0.11091182], Correct: 535, Train-Acc: 0.530\n",
      "Iter: 2009, Trn-Error: [0.31535068], Correct: 1287, Train-Acc: 0.640\n",
      "Iter: 3009, Trn-Error: [0.0547328], Correct: 2114, Train-Acc: 0.702\n",
      "Iter: 4009, Trn-Error: [0.13114956], Correct: 2915, Train-Acc: 0.727\n",
      "Iter: 5009, Trn-Error: [0.00477812], Correct: 3715, Train-Acc: 0.742\n",
      "Iter: 6009, Trn-Error: [0.44216895], Correct: 4536, Train-Acc: 0.755\n",
      "Iter: 7009, Trn-Error: [0.00066642], Correct: 5390, Train-Acc: 0.769\n",
      "Iter: 8009, Trn-Error: [0.00644237], Correct: 6255, Train-Acc: 0.781\n",
      "Iter: 9009, Trn-Error: [0.86182665], Correct: 7107, Train-Acc: 0.789\n",
      "Iter: 10009, Trn-Error: [0.48895955], Correct: 7973, Train-Acc: 0.797\n",
      "Iter: 11009, Trn-Error: [0.0004263], Correct: 8825, Train-Acc: 0.802\n",
      "Iter: 12009, Trn-Error: [2.91075966e-06], Correct: 9672, Train-Acc: 0.805\n",
      "Iter: 13009, Trn-Error: [0.0006242], Correct: 10547, Train-Acc: 0.811\n",
      "Iter: 14009, Trn-Error: [0.11730786], Correct: 11399, Train-Acc: 0.814\n",
      "Iter: 15009, Trn-Error: [0.00028675], Correct: 12239, Train-Acc: 0.815\n",
      "Iter: 16009, Trn-Error: [0.04678272], Correct: 13068, Train-Acc: 0.816\n",
      "Iter: 17009, Trn-Error: [0.00059001], Correct: 13923, Train-Acc: 0.819\n",
      "Iter: 18009, Trn-Error: [0.03916666], Correct: 14785, Train-Acc: 0.821\n",
      "Iter: 19009, Trn-Error: [0.00554911], Correct: 15648, Train-Acc: 0.823\n",
      "Iter: 20009, Trn-Error: [0.00613193], Correct: 16527, Train-Acc: 0.826\n",
      "Iter: 21009, Trn-Error: [0.1378773], Correct: 17398, Train-Acc: 0.828\n",
      "Iter: 22009, Trn-Error: [9.06769557e-05], Correct: 18273, Train-Acc: 0.830\n",
      "Iter: 23009, Trn-Error: [0.30084703], Correct: 19134, Train-Acc: 0.832\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(input_dataset)-1000): #first 24k reviews\n",
    "    \n",
    "    error = 0\n",
    "    x,y = (input_dataset[i], target_dataset[i])\n",
    "    layer_1 = sigmoid(np.sum(w01[x], axis = 0)) #sum across the 4 rows of weights, will always be (100,)\n",
    "    layer_2 = sigmoid(np.dot(layer_1, w12)) #(1)\n",
    "    \n",
    "    error += (layer_2 - y)**2\n",
    "    \n",
    "    layer_2_delta = layer_2 - y\n",
    "    layer_1_delta = layer_2_delta.dot(w12.T) #? no derivative ? (100,)\n",
    "    \n",
    "    w01[x] -= layer_1_delta* alpha #only update the relevant weights\n",
    "    w12 -= np.outer(layer_1, layer_2_delta) * alpha\n",
    "    \n",
    "    if(np.abs(layer_2_delta) < 0.5):\n",
    "        correct += 1\n",
    "    total +=1\n",
    "    \n",
    "    if(i % 1000 == 9):\n",
    "        print(f\"Iter: {i}, Trn-Error: {error}, Correct: {correct}, Train-Acc: {correct/float(total):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Correct 850, Test Accuracy 0.850.\n"
     ]
    }
   ],
   "source": [
    "correct, total = (0,0)\n",
    "for i in range(len(input_dataset) - 1000, len(input_dataset)):\n",
    "    \n",
    "    x = input_dataset[i]\n",
    "    y = target_dataset[i]\n",
    "    \n",
    "    layer_1 = sigmoid(np.sum(w01[x], axis = 0))\n",
    "    layer_2 = sigmoid(np.dot(layer_1, w12))\n",
    "    \n",
    "    if(np.abs(layer_2 -y) < 0.5):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    \n",
    "print(f\"Test Correct {correct}, Test Accuracy {correct/float(total):.3f}.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_brave = w01[16240]\n",
    "w_beautiful = w01[21333]\n",
    "square_diff = sum((w_brave - w_beautiful)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0330431891639722"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "-math.sqrt(square_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def similar(target = \"brave\"):\n",
    "    scores = Counter()\n",
    "    target_index = word2index[target] #16420\n",
    "    for word, index in word2index.items():\n",
    "        raw_diff = w01[index] - w01[target_index] #100 - 100\n",
    "        square_diff = raw_diff * raw_diff #square the diff\n",
    "        scores[word] = -math.sqrt(sum(square_diff)) #sqrt the sum of all the diff, using a counter here and then most common \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "brave_score = similar(\"brave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brave', -0.0),\n",
       " ('ramone', -0.6518620020234145),\n",
       " ('cautions', -0.6570388689372177),\n",
       " ('culminating', -0.660379190166895),\n",
       " ('risdon', -0.664876917580303),\n",
       " ('familiarly', -0.6661246325585951),\n",
       " ('ncos', -0.6673909117361397),\n",
       " ('scorpion', -0.6679715838956195),\n",
       " ('wobbles', -0.6689169531906572),\n",
       " ('maniquen', -0.6703335445914319)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brave_score.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
